# Text to Image Search Engine with_Sceneraphs
Recent advancements in Vision-Language Models (VLMs) have significantly improved image retrieval systems, enabling more nuanced matching between textual queries and visual content. However, current models still struggle with accurately interpreting and retrieving images based on complex spatial relationships and object interactions described in multi-part queries. We focus on overcoming the tendency of existing models to behave as ``bags-of-words," which often results in misinterpretation of spatial and contextual relationships in complex queries. This limitation hinders the effectiveness of image search engines in scenarios requiring precise compositional understanding, such as e-commerce, healthcare, and education. We propose a novel framework that fuses scene graph embeddings, derived from Graph Neural Networks (GNNs), with text and image embeddings from VLMs. This fusion improves the relational information of the embeddings, thereby improving the model's ability to retrieve images that accurately match complex, multi-part queries. Our approach shows a significant improvement of 4â€“5\% in retrieval accuracy.

 ![image](https://github.com/user-attachments/assets/8fd177a3-13b2-42f3-a998-a2e4ff432b65)
 <p align="center">Methodology for text-to-image retrieval using Scenegraph embeddings. The system combines image embeddings from a CLIP Image Encoder with Scenegraph embeddings generated from the image dataset to produce fused image embeddings, which are indexed using Annoy trees for efficient similarity search. For the example text query, ``person with dark pants sitting beside a clear window,'' the CLIP Text Encoder generates embeddings that are fused with Scenegraph embeddings from the query. This enables approximate nearest neighbor search, retrieving the top-ranked images, such as a person sitting by a window, aligning closely with the query description..</p>
