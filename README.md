# Text to Image Search Engine with_Sceneraphs
Recent advancements in Vision-Language Models (VLMs) have significantly improved image retrieval systems, enabling more nuanced matching between textual queries and visual content. However, current models still struggle with accurately interpreting and retrieving images based on complex spatial relationships and object interactions described in multi-part queries. We focus on overcoming the tendency of existing models to behave as ``bags-of-words," which often results in misinterpretation of spatial and contextual relationships in complex queries. This limitation hinders the effectiveness of image search engines in scenarios requiring precise compositional understanding, such as e-commerce, healthcare, and education. We propose a novel framework that fuses scene graph embeddings, derived from Graph Neural Networks (GNNs), with text and image embeddings from VLMs. This fusion improves the relational information of the embeddings, thereby improving the model's ability to retrieve images that accurately match complex, multi-part queries. Our approach shows a significant improvement of 4â€“5\% in retrieval accuracy.

 ![image](https://github.com/user-attachments/assets/8fd177a3-13b2-42f3-a998-a2e4ff432b65)
