{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/xx/cq2k_b4j60l3mtp2k3x41s0w0000gn/T/ipykernel_44606/3536710385.py:115: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(\"gnn_model/model_checkpoint.pth\")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import dgl\n",
    "from dgl.nn import GraphConv\n",
    "from factual_scene_graph.parser.scene_graph_parser import SceneGraphParser\n",
    "import json\n",
    "\n",
    "parser = SceneGraphParser('lizhuang144/flan-t5-base-VG-factual-sg', device='cpu')\n",
    "\n",
    "captions_text_graph = {\n",
    "    \"552666\": [(\"people\" , \"is\" , \"group of\") , ( \"people\" , \"ride\" , \"horse\" ) , ( \"people\" , \"ride through\" , \"field\" )],\n",
    "    \"687618\": [( \"man\" , \"stand in\" ,\"shirt\" ) , ( \"shirt\" , \"is\" , \"blue\" )],\n",
    "    \"405058\": [( \"counters\" , \"is\" , \"wooden\" ) , ( \"kitchen\" , \"is\" , \"simple\" ) , ( \"kitchen\" , \"with\" , \"stove\" )],\n",
    "    \"703860\": [( \"adult\" , \"ride\" , \"motorcycle\" ) , ( \"adult\" , \"ride with\" , \"child\" ) , ( \"adults\" , \"ride\" , \"motorcycle\" )],\n",
    "    \"776132\": [( \"kite\" , \"fly over\" , \"sky\" ) , ( \"kite\" , \"is\" , \"yellow\" ) , ( \"sky\" , \"is\" , \"large\" )],\n",
    "    \"287571\": [( \"skate boarder\" , \"trick on\" , \"picnic table\" )],\n",
    "    \"67000\": [( \"elephant\" , \"inside\" , \"fence\" ) , ( \"fence\" , \"is\" , \"wire\" ) , ( \"people\" , \"on side of\" , \"fence\" )],\n",
    "    \"137494\": [( \"slices\" , \"eat from\" , \"pizza\" )],\n",
    "    \"427130\": [( \"dog\" , \"is\" , \"brown\" ) , ( \"dog\" , \"walk across\" , \"field\" ) , ( \"field\" , \"is\" , \"green\" ) , ( \"frisbee\", \"in\" , \"mouth\" )],\n",
    "    \"285328\": [( \"men\" , \"at\" , \"stop light\" ) , ( \"men\" , \"is\" , \"2\" ) , ( \"men\" , \"on\" , \"motorcycles\" )],\n",
    "    \"549270\": [( \"silverware\" , \"have\" , \"handle\" )],\n",
    "    \"270030\": [( \"area\" , \"is\" , \"grassy\" ) , ( \"zebras\" , \"is\" , \"4\" ) , ( \"zebras\" , \"walk in\" , \"area\" )],\n",
    "    \"377603\": [( \"cow\" , \"is\" , \"statue\" ) , ( \"dog\" , \"look at\" , \"cow\" )],\n",
    "    \"22348\": [( \"field\" , \"next to\" ,\" forest \") , ( \"horses\" , \"graze in\" , \"field\" ) , ( \"horses\" , \"is\" , \"2\" )],\n",
    "    \"196658\": [( \"boy\" , \"in\" , \"helmet\" ) , ( \"boy\" , \"ride\" , \"skateboard\" )],\n",
    "    \"231576\": [( \"mobile phone\" , \"on\" , \"display area\" ) , ( \"mobile phone\" , \"with\" , \"writing\" ) , ( \"writing\" , \"is\" , \"Asian\" )],\n",
    "    \"491426\": [( \"flowers\" , \"next to\" , \"vase\" ) , ( \"pictures\" , \"next to\" , \"vase\" ) , ( \"vase\" , \"is\" , \"beautiful\" )],\n",
    "    \"455622\": [( \"pizza\" , \"is\" , \"piece\" ) , ( \"plate\" , \"have\" , \"fork\" ) , ( \"plate\" , \"have\" , \"pizza\" )],\n",
    "    \"452624\": [( \"boy\" , \"hold\" , \"tennis racquet\" ) , ( \"boy\" , \"on\" , \"court\" )],\n",
    "    \"587826\": [( \"man\" , \"in\" , \"parade\" ) , ( \"man\" , \"on\" , \"motorcycle\" )]\n",
    "}\n",
    "\n",
    "class SceneGraphGNN(nn.Module):\n",
    "    def __init__(self, in_feats, hidden_feats, num_classes):\n",
    "        super(SceneGraphGNN, self).__init__()\n",
    "        self.conv1 = GraphConv(in_feats, hidden_feats)  # First GraphConv layer\n",
    "        self.conv2 = GraphConv(hidden_feats, num_classes)  # Second GraphConv layer\n",
    "        self.relu = nn.ReLU()  # Activation function (ReLU)\n",
    "\n",
    "    def forward(self, g, features):\n",
    "        # Graph Convolutional Layers\n",
    "        h = self.conv1(g, features)  # Apply first graph convolution\n",
    "        h = self.relu(h)  # Apply ReLU activation\n",
    "        h = self.conv2(g, h)  # Apply second graph convolution\n",
    "        return h  # Return the output features (node embeddings)\n",
    "    \n",
    "def process_scene_graph(scene_graph):\n",
    "    \"\"\"\n",
    "    Convert a scene graph in textual form into nodes and edges.\n",
    "    \"\"\"\n",
    "    nodes = set()\n",
    "    edges = []\n",
    "    # print(\"Raw scene_graph:\", scene_graph)\n",
    "    # Parse the scene graph\n",
    "    for relation in scene_graph:\n",
    "        # print(\"relation:\", relation)\n",
    "        # print(\"len(relation)\",len(relation))\n",
    "        if len(relation) < 3:\n",
    "            continue\n",
    "        for i in range(0,len(relation),3):\n",
    "            src = relation[i].strip().lstrip(\"(\").strip()\n",
    "            rel = relation[i + 1].strip().lstrip('v:').strip()\n",
    "            dst = relation[i + 2].strip().rstrip(\")\").strip()\n",
    "            # print(f\"Processed relation: src = {src}, rel = {rel}, dst = {dst}\")\n",
    "            nodes.add(src)\n",
    "            nodes.add(dst)\n",
    "            edges.append((src, rel, dst))\n",
    "\n",
    "    return list(nodes), edges\n",
    "\n",
    "def create_dgl_graph(nodes, edges):\n",
    "    \"\"\"\n",
    "    Create a DGL graph from nodes and edges.\n",
    "    \"\"\"\n",
    "    # Map nodes and relations to unique IDs\n",
    "    # print(\"nodes:\", nodes)\n",
    "    # print(\"edges:\", edges)\n",
    "    node_to_id = {node: i for i, node in enumerate(nodes)}\n",
    "    # print(\"node_to_id\", node_to_id)\n",
    "    relation_to_id = {rel: i for i, (_, rel, _) in enumerate(edges)}\n",
    "    # print(\"relation_to_id:\", relation_to_id)\n",
    "\n",
    "    # Prepare source and destination node indices\n",
    "    src_nodes = [node_to_id[src] for src, _, _ in edges]\n",
    "    # print(\"src_nodes:\", src_nodes)\n",
    "    dst_nodes = [node_to_id[dst] for _, _, dst in edges]\n",
    "    # print(\"dst_nodes:\", dst_nodes)\n",
    "\n",
    "    # Create the graph\n",
    "    g = dgl.graph((src_nodes, dst_nodes), num_nodes=len(nodes))\n",
    "    # print(\"g (before adding self-loops):\", g)\n",
    "\n",
    "    # Add self-loops to the graph\n",
    "    g = dgl.add_self_loop(g)\n",
    "    # print(\"g (after adding self-loops):\", g)\n",
    "\n",
    "    # Update edge features to match the new number of edges\n",
    "    original_edge_features = [relation_to_id[rel] for _, rel, _ in edges]\n",
    "    self_loop_features = [-1] * len(nodes)  # Assign default value for self-loops\n",
    "    all_edge_features = original_edge_features + self_loop_features\n",
    "    g.edata['relation_type'] = torch.tensor(all_edge_features, dtype=torch.int64)\n",
    "    # print(\"g with edge data:\", g)\n",
    "\n",
    "    return g, node_to_id, relation_to_id\n",
    "\n",
    "input_dim = 64  # Input node feature dimension\n",
    "hidden_dim = 128  # Hidden layer dimension\n",
    "num_classes = 512  # Number of output classes (adjust based on your task)\n",
    "\n",
    "# Initialize the model\n",
    "model = SceneGraphGNN(input_dim, hidden_dim, num_classes)\n",
    "\n",
    "# Load checkpoint\n",
    "checkpoint = torch.load(\"gnn_model/model_checkpoint.pth\")\n",
    "\n",
    "# Access model and optimizer states\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "model.eval()\n",
    "\n",
    "captions_graph_embeddings = {}\n",
    "\n",
    "for key, converted_scene_graph in captions_text_graph.items():\n",
    "    nodes , edges = process_scene_graph(converted_scene_graph)\n",
    "    g, node_to_id, relation_to_id= create_dgl_graph(nodes , edges)\n",
    "    g.ndata['feat'] = torch.rand(len(nodes), 64)  # Random 10-dimensional features\n",
    "    logits = model(g, g.ndata['feat'])\n",
    "    captions_graph_embeddings[key] = logits[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kaushalpatil/Development/USC MS CSAI Program/Deep Learning and its Applications/venv/lib/python3.10/site-packages/dgl/backend/pytorch/tensor.py:352: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  assert input.numel() == input.storage().size(), \"Cannot convert view \" \\\n"
     ]
    }
   ],
   "source": [
    "captions_graph_embeddings = {}\n",
    "\n",
    "for key, converted_scene_graph in captions_text_graph.items():\n",
    "    nodes , edges = process_scene_graph(converted_scene_graph)\n",
    "    g, node_to_id, relation_to_id= create_dgl_graph(nodes , edges)\n",
    "    g.ndata['feat'] = torch.rand(len(nodes), 64)  # Random 10-dimensional features\n",
    "    logits = model(g, g.ndata['feat'])\n",
    "    captions_graph_embeddings[key] = logits[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(captions_graph_embeddings['552666'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['RN50',\n",
       " 'RN101',\n",
       " 'RN50x4',\n",
       " 'RN50x16',\n",
       " 'RN50x64',\n",
       " 'ViT-B/32',\n",
       " 'ViT-B/16',\n",
       " 'ViT-L/14',\n",
       " 'ViT-L/14@336px']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from pkg_resources import packaging\n",
    "from transformers import CLIPProcessor, CLIPModel, CLIPTokenizer\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from chromadb.config import Settings\n",
    "from annoy import AnnoyIndex\n",
    "import json\n",
    "import numpy as np\n",
    "# print(\"Torch version:\", torch.__version__)\n",
    "\n",
    "import clip\n",
    "\n",
    "clip.available_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters: 151,277,313\n",
      "Input resolution: 224\n",
      "Context length: 77\n",
      "Vocab size: 49408\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CLIP(\n",
       "  (visual): VisionTransformer(\n",
       "    (conv1): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32), bias=False)\n",
       "    (ln_pre): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (transformer): Transformer(\n",
       "      (resblocks): Sequential(\n",
       "        (0): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (1): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (2): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (3): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (4): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (5): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (6): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (7): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (8): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (9): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (10): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (11): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_post): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (transformer): Transformer(\n",
       "    (resblocks): Sequential(\n",
       "      (0): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (1): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (2): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (3): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (4): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (5): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (6): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (7): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (8): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (9): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (10): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (11): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (token_embedding): Embedding(49408, 512)\n",
       "  (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model, preprocess = clip.load(\"ViT-B/32\")\n",
    "# model.cuda().eval()\n",
    "input_resolution = model.visual.input_resolution\n",
    "context_length = model.context_length\n",
    "vocab_size = model.vocab_size\n",
    "\n",
    "print(\"Model parameters:\", f\"{np.sum([int(np.prod(p.shape)) for p in model.parameters()]):,}\")\n",
    "print(\"Input resolution:\", input_resolution)\n",
    "print(\"Context length:\", context_length)\n",
    "print(\"Vocab size:\", vocab_size)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_embedding(image_path):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    preprocessed = preprocess(image)\n",
    "    image_input = torch.tensor(preprocessed).unsqueeze(0)  # Add batch dimension\n",
    "    with torch.no_grad():\n",
    "        image_features = model.encode_image(image_input).float()\n",
    "    image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "    return image_features\n",
    "\n",
    "def get_text_embedding(text, model=model, device=device):\n",
    "    input_token = clip.tokenize(text).to(device)\n",
    "    with torch.no_grad():\n",
    "        text_features = model.encode_text(input_token).float()\n",
    "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "    return text_features\n",
    "\n",
    "def cosine_similarity(a, b):\n",
    "    dot_product = np.dot(a, b)\n",
    "    norm_a = np.linalg.norm(a)\n",
    "    norm_b = np.linalg.norm(b)\n",
    "    return dot_product / (norm_a * norm_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    }
   ],
   "source": [
    "with open('output.json', 'r') as file:\n",
    "    images = json.load(file)\n",
    "print(len(images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'552666': ['000000445512', '000000068120', '000000439180', '000000424812', '000000322511', '000000560880', '000000441009', '000000031667', '000000450037', '000000474934', '000000329336', '000000492282', '000000373119'], '687618': ['000000143554', '000000084258', '000000296524', '000000032400', '000000240274', '000000515347', '000000042201', '000000333182', '000000262175'], '405058': ['000000301827', '000000545734', '000000482022', '000000575367', '000000048905', '000000206705', '000000412978', '000000059383', '000000097240', '000000298461'], '703860': ['000000269254', '000000191846', '000000012744', '000000559665', '000000187857', '000000480021', '000000005205', '000000031965', '000000349437', '000000487450', '000000046077'], '776132': ['000000462755', '000000182279', '000000276458', '000000502090', '000000082293', '000000509270', '000000382743'], '287571': ['000000256868', '000000262148', '000000391365', '000000208808', '000000125071', '000000110551', '000000465911', '000000532211', '000000360181', '000000208311'], '67000': ['000000368961', '000000259625', '000000392212', '000000533979', '000000134555', '000000273118'], '137494': ['000000365121', '000000538819', '000000044934', '000000545039', '000000520787', '000000539926', '000000163611', '000000110630', '000000182503', '000000267694', '000000294831', '000000207151', '000000384822', '000000039671', '000000490620', '000000132415'], '427130': ['000000300000', '000000002753', '000000104421', '000000025989', '000000347950', '000000421072', '000000169361', '000000161144', '000000403421'], '285328': ['000000019904', '000000332877', '000000508302', '000000339022', '000000559665', '000000024343', '000000031965', '000000046077'], '549270': ['000000016928', '000000305540', '000000443949', '000000550322', '000000538451', '000000437599'], '270030': ['000000132612', '000000391016', '000000266920', '000000299089', '000000456690', '000000100661', '000000398005', '000000572055'], '377603': ['000000178761', '000000279994', '000000090366'], '22348': ['000000303267', '000000248774', '000000581929', '000000068490', '000000126766', '000000546095', '000000051250', '000000367579', '000000074460', '000000529981'], '196658': ['000000519744', '000000230884', '000000124647', '000000526126', '000000229553', '000000126257', '000000384723', '000000465911', '000000046269', '000000442463'], '231576': ['000000154241', '000000334977', '000000498665', '000000436426', '000000444746', '000000191069'], '491426': ['000000461378', '000000077282', '000000366630', '000000251590', '000000027656', '000000410278', '000000478282', '000000213224', '000000447087', '000000547601', '000000541587', '000000083923', '000000393338'], '455622': ['000000365121', '000000069698', '000000044934', '000000110630', '000000182503', '000000294831', '000000520787', '000000095062', '000000163611', '000000132415'], '452624': ['000000372384', '000000258628', '000000272262', '000000015017', '000000325228', '000000280909', '000000069392', '000000273493', '000000323639', '000000246649', '000000258588', '000000134558'], '587826': ['000000019904', '000000225792', '000000312803', '000000580613', '000000196490', '000000253964', '000000171695', '000000559665', '000000445041', '000000187857', '000000046077']}\n"
     ]
    }
   ],
   "source": [
    "print(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "# Specify the folder path\n",
    "combined_dict = {}\n",
    "\n",
    "for folder in images:\n",
    "    folder_path = f'/Users/kaushalpatil/Development/USC MS CSAI Program/Deep Learning and its Applications/image retrieval dataset/{str(folder)}'\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.lower().endswith(('.png', '.jpg', '.jpeg', '.gif')):\n",
    "            image_path = os.path.join(folder_path, filename)\n",
    "            img = Image.open(image_path)\n",
    "            # print(filename[18:-4])\n",
    "            id = int(folder + filename[18:-4])\n",
    "            # # print(image_path)\n",
    "            # # (type(id))\n",
    "            combined_dict[id] = image_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/xx/cq2k_b4j60l3mtp2k3x41s0w0000gn/T/ipykernel_44606/2139394116.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  image_input = torch.tensor(preprocessed).unsqueeze(0)  # Add batch dimension\n"
     ]
    }
   ],
   "source": [
    "embeddings_combined_dict = {}\n",
    "\n",
    "for folder in images:\n",
    "    folder_path = f'/Users/kaushalpatil/Development/USC MS CSAI Program/Deep Learning and its Applications/image retrieval dataset/{str(folder)}'\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.lower().endswith(('.png', '.jpg', '.jpeg', '.gif')):\n",
    "            image_path = os.path.join(folder_path, filename)\n",
    "            img = Image.open(image_path)\n",
    "            id = int(folder + filename[18:-4])\n",
    "            basic_embeddings = get_image_embedding(image_path)[0]\n",
    "            gnn_embeddings = captions_graph_embeddings[folder]\n",
    "            embeddings_combined_dict[id] = result = torch.cat((basic_embeddings.unsqueeze(0), gnn_embeddings.unsqueeze(0)), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "alternate_keys = {}\n",
    "start = 1\n",
    "for i in embeddings_combined_dict:\n",
    "    alternate_keys[i] = start\n",
    "    start += 1\n",
    "    \n",
    "reverse_alternate_keys = {}\n",
    "\n",
    "for i, j in alternate_keys.items():\n",
    "    reverse_alternate_keys[j] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/xx/cq2k_b4j60l3mtp2k3x41s0w0000gn/T/ipykernel_44606/1117058371.py:2: FutureWarning: The default argument for metric will be removed in future version of Annoy. Please pass metric='angular' explicitly.\n",
      "  t = AnnoyIndex(f)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = 1024 # Number of Dimensions\n",
    "t = AnnoyIndex(f)\n",
    "for i, j in embeddings_combined_dict.items():\n",
    "    t.add_item(alternate_keys[i], j[0]) # Adding the key-value pair of the AnnoyIndex\n",
    "\n",
    "t.build(f) # Building 1024 trees for AnnoyIndex, more the number of trees, more the memory consumed, better are the results of ANN algorithm\n",
    "t.save('image-search-tree-gnn-rn50.ann') # Saving the AnnoyIndex for faster reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/xx/cq2k_b4j60l3mtp2k3x41s0w0000gn/T/ipykernel_44606/1905123095.py:1: FutureWarning: The default argument for metric will be removed in future version of Annoy. Please pass metric='angular' explicitly.\n",
      "  search_space = AnnoyIndex(1024)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_space = AnnoyIndex(1024)\n",
    "search_space.load('./image-search-tree-gnn-rn50.ann')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_image_search(folder, query : str, num : int = 10):\n",
    "    query_vector = get_text_embedding(query)[0]\n",
    "    gnn_embeddings = captions_graph_embeddings[folder]\n",
    "    query_embeddings = torch.cat((query_vector.unsqueeze(0), gnn_embeddings.unsqueeze(0)), dim=1)\n",
    "    ans = search_space.get_nns_by_vector(query_embeddings[0], num)\n",
    "    return ans\n",
    "\n",
    "def recall_at_k(actual, predicted, k):\n",
    "    actual_set = set(actual)\n",
    "    top_k_predicted = set(predicted[:k])\n",
    "    relevant_hits = len(actual_set & top_k_predicted)\n",
    "    if len(actual_set) == 0:\n",
    "        return 0.0\n",
    "    return round(relevant_hits / len(actual_set), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('new-captions.json', 'r') as file:\n",
    "    captions = json.load(file)\n",
    "    \n",
    "with open('output.json', 'r') as file:\n",
    "    actual_images = json.load(file)\n",
    "    \n",
    "answers = {}\n",
    "finals = {1: [], 2: [], 5: [], 10: []}\n",
    "for i, caption in captions.items():\n",
    "    ans = text_image_search(query=caption, folder=i)\n",
    "    predicted = []\n",
    "    for p in ans:\n",
    "        key = reverse_alternate_keys[p]\n",
    "        predicted.append(key)\n",
    "    ls = actual_images[i]\n",
    "    actual = [int(i+k[-6:]) for k in ls]\n",
    "    answers[i] = []\n",
    "    for val in [1, 2, 5, 10]:\n",
    "        temp = recall_at_k(actual, predicted, val)\n",
    "        finals[val].append(temp)\n",
    "        answers[i].append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 0.112, 2: 0.203, 5: 0.439, 10: 0.719}\n"
     ]
    }
   ],
   "source": [
    "ekdum_final = {}\n",
    "for i, ls in finals.items():\n",
    "    ekdum_final[i] = round(sum(ls) / len(ls), 5)\n",
    "    # ekdum_final[i] = % .4f % ekdum_final[i]\n",
    "print(ekdum_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'552666': [0.0, 0.0, 0.23, 0.31],\n",
       " '687618': [0.11, 0.22, 0.56, 0.89],\n",
       " '405058': [0.1, 0.2, 0.5, 1.0],\n",
       " '703860': [0.09, 0.18, 0.27, 0.45],\n",
       " '776132': [0.14, 0.29, 0.71, 0.86],\n",
       " '287571': [0.1, 0.2, 0.5, 0.8],\n",
       " '67000': [0.17, 0.33, 0.67, 0.67],\n",
       " '137494': [0.06, 0.06, 0.19, 0.31],\n",
       " '427130': [0.11, 0.22, 0.56, 1.0],\n",
       " '285328': [0.12, 0.12, 0.25, 0.62],\n",
       " '549270': [0.17, 0.33, 0.67, 0.67],\n",
       " '270030': [0.12, 0.25, 0.5, 1.0],\n",
       " '377603': [0.33, 0.33, 0.33, 1.0],\n",
       " '22348': [0.1, 0.2, 0.5, 0.9],\n",
       " '196658': [0.1, 0.2, 0.4, 0.5],\n",
       " '231576': [0.17, 0.33, 0.67, 0.83],\n",
       " '491426': [0.08, 0.15, 0.38, 0.77],\n",
       " '455622': [0.0, 0.1, 0.2, 0.5],\n",
       " '452624': [0.08, 0.17, 0.42, 0.75],\n",
       " '587826': [0.09, 0.18, 0.27, 0.55]}"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
