{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import open_clip\n",
    "import torch\n",
    "import numpy as np\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('RN50', 'openai'),\n",
       " ('RN50', 'yfcc15m'),\n",
       " ('RN50', 'cc12m'),\n",
       " ('RN101', 'openai'),\n",
       " ('RN101', 'yfcc15m')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "open_clip.list_pretrained()[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ViT-B-32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Winoground_accuracies': {'text_score': 0.305,\n",
       "  'image_score': 0.095,\n",
       "  'group_score': 0.075}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from winoground_evaluation import Winoground_evaluation\n",
    "# you can either do it by calling the evaluate open clip function\n",
    "# winoground_accuracy_values = evaluate_open_clip_on_winoground(model_name='ViT-B-32', pretrained=\"laion2b_s34b_b79k\")\n",
    "model_name='ViT-B-32'\n",
    "pretrained=\"openai\"\n",
    "win_eval = Winoground_evaluation(model_name, pretrained)\n",
    "\n",
    "winoground_accuracy_values = win_eval.evaluate_open_clip_on_winoground()\n",
    "winoground_accuracy_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ViT-L-14 - OG CLIP Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Winoground_accuracies': {'text_score': 0.3025,\n",
       "  'image_score': 0.0925,\n",
       "  'group_score': 0.0675}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from winoground_evaluation import Winoground_evaluation\n",
    "# you can either do it by calling the evaluate open clip function\n",
    "# winoground_accuracy_values = evaluate_open_clip_on_winoground(model_name='ViT-B-32', pretrained=\"laion2b_s34b_b79k\")\n",
    "model_name='ViT-L-14'\n",
    "pretrained=\"openai\"\n",
    "win_eval = Winoground_evaluation(model_name, pretrained)\n",
    "\n",
    "winoground_accuracy_values = win_eval.evaluate_open_clip_on_winoground()\n",
    "winoground_accuracy_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "782d339b179d4c7e92192e22e3c3a07e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "open_clip_model.safetensors:   0%|          | 0.00/408M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'Winoground_accuracies': {'text_score': 0.145,\n",
       "  'image_score': 0.0625,\n",
       "  'group_score': 0.02}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from winoground_evaluation import Winoground_evaluation\n",
    "# you can either do it by calling the evaluate open clip function\n",
    "# winoground_accuracy_values = evaluate_open_clip_on_winoground(model_name='ViT-B-32', pretrained=\"laion2b_s34b_b79k\")\n",
    "model_name='RN50'\n",
    "pretrained=\"openai\"\n",
    "win_eval = Winoground_evaluation(model_name, pretrained)\n",
    "\n",
    "winoground_accuracy_values = win_eval.evaluate_open_clip_on_winoground()\n",
    "winoground_accuracy_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "325ae231f813496481fe86ec2468593e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "open_clip_model.safetensors:   0%|          | 0.00/479M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'Winoground_accuracies': {'text_score': 0.1425,\n",
       "  'image_score': 0.0375,\n",
       "  'group_score': 0.0225}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from winoground_evaluation import Winoground_evaluation\n",
    "# you can either do it by calling the evaluate open clip function\n",
    "# winoground_accuracy_values = evaluate_open_clip_on_winoground(model_name='ViT-B-32', pretrained=\"laion2b_s34b_b79k\")\n",
    "model_name='RN101'\n",
    "pretrained=\"openai\"\n",
    "win_eval = Winoground_evaluation(model_name, pretrained)\n",
    "\n",
    "winoground_accuracy_values = win_eval.evaluate_open_clip_on_winoground()\n",
    "winoground_accuracy_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40f3db0aabac4c7d84dd40b7b0ecd103",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "open_clip_model.safetensors:   0%|          | 0.00/599M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'Winoground_accuracies': {'text_score': 0.2425,\n",
       "  'image_score': 0.0825,\n",
       "  'group_score': 0.0675}}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from winoground_evaluation import Winoground_evaluation\n",
    "# you can either do it by calling the evaluate open clip function\n",
    "# winoground_accuracy_values = evaluate_open_clip_on_winoground(model_name='ViT-B-32', pretrained=\"laion2b_s34b_b79k\")\n",
    "model_name='ViT-B-16'\n",
    "pretrained=\"openai\"\n",
    "win_eval = Winoground_evaluation(model_name, pretrained)\n",
    "\n",
    "winoground_accuracy_values = win_eval.evaluate_open_clip_on_winoground()\n",
    "winoground_accuracy_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import json\n",
    "from winoground_evaluation import Winoground_generative_evaluation\n",
    "from transformers import AutoProcessor, LlavaForConditionalGeneration\n",
    "from transformers import Blip2Processor, Blip2ForConditionalGeneration, Blip2Model, AutoTokenizer\n",
    "from transformers import LlamaTokenizer, AutoModelForCausalLM\n",
    "import wandb\n",
    "import torch\n",
    "from torchvision import transforms as T\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser(description=__doc__)\n",
    "_AA = parser.add_argument\n",
    "\n",
    "_AA(\"--model_list\", nargs='+', help=\"List of models to evaluate.\")\n",
    "_AA(\"--benchmark_list\", nargs='+', default=None, help=\"List of benchmarks to evaluate models on.\")\n",
    "_AA(\"--prompt_type_list\", nargs='+', default=None, help=\"List of prompt types.\")\n",
    "_AA(\"--sugarcrepe_subsets\", nargs='+', default=None, help=\"List of SugarCrepe subsets to evaluate.\")\n",
    "_AA(\"--aro_subsets\", nargs='+', default=None, help=\"List of ARO subsets to evaluate.\")\n",
    "_AA(\"--evaluation_type\", help=\"Evaluation mode to activate. Accuracy overall or text/image/group scores.\")\n",
    "_AA(\"--no_hard_negatives\", help=\"Evaluation mode in which caption and image pairs are swapped with ones from different examples.\")\n",
    "_AA(\"--tryingout_ce\", default=False, help=\"Tryingout for contrastive evaluation.\")\n",
    "\n",
    "BENCHMARKS_LIST = [\"winoground\"]\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
