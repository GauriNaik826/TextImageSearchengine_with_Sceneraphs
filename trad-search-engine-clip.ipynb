{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install chromadb\n",
    "!pip install numpy pandas transformers\n",
    "!pip install pillow tqdm torch annoy datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e78a10a59b44862948dae41055e7ce4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import snapshot_download, login\n",
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import CLIPProcessor, CLIPModel, CLIPTokenizer\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from chromadb.config import Settings\n",
    "import chromadb\n",
    "import requests\n",
    "from annoy import AnnoyIndex\n",
    "import json\n",
    "import numpy as np\n",
    "from imageinterminal import display_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"openai/clip-vit-base-patch32\"\n",
    "model = CLIPModel.from_pretrained(model_name)\n",
    "processor = CLIPProcessor.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "tokenizer = CLIPTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embeddings Related Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_embedding(image_path):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    inputs = processor(images=image, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        image_features = model.get_image_features(**inputs)\n",
    "    image_embedding = image_features / image_features.norm(dim=-1, keepdim=True)\n",
    "    return image_embedding.cpu().numpy().squeeze()\n",
    "\n",
    "def get_text_embedding(text, model=model, tokenizer=tokenizer, device=device):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "    with torch.no_grad():\n",
    "        text_features = model.get_text_features(**inputs)\n",
    "    text_embedding = text_features / text_features.norm(dim=-1, keepdim=True)\n",
    "    \n",
    "    return text_embedding.cpu().numpy().squeeze()\n",
    "\n",
    "def cosine_similarity(a, b):\n",
    "    dot_product = np.dot(a, b)\n",
    "    norm_a = np.linalg.norm(a)\n",
    "    norm_b = np.linalg.norm(b)\n",
    "    return dot_product / (norm_a * norm_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample check of CLIP's embeddings capability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float32(0.30010346)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings = get_image_embedding('horse.png')\n",
    "text_embeddings = get_text_embedding(\"a black and white image of horse\", model, tokenizer, device)\n",
    "cosine_similarity(embeddings, text_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forming Search Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/examples.jsonl', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "combined_dict = {}\n",
    "for i in range(400):\n",
    "    first_image = data[i]['image_0']\n",
    "    first_key = str(i) + '0'\n",
    "    second_image = data[i]['image_1']\n",
    "    second_key = str(i) + '1'\n",
    "    combined_dict[int(first_key)] =  get_image_embedding('./data/images/images_' + first_image + '.png')\n",
    "    combined_dict[int(second_key)] =  get_image_embedding('./data/images/images_' + second_image + '.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_combined_dict = {}\n",
    "for i in range(400):\n",
    "    first_image = data[i]['image_0']\n",
    "    first_key = str(i) + '0'\n",
    "    second_image = data[i]['image_1']\n",
    "    second_key = str(i) + '1'\n",
    "    first_caption = data[i]['caption_0']\n",
    "    second_caption = data[i]['caption_1']\n",
    "    all_combined_dict[int(first_key)] =  ['./data/images/images_' + first_image + '.png', first_caption]\n",
    "    all_combined_dict[int(second_key)] =  ['./data/images/images_' + second_image + '.png', second_caption]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/xx/cq2k_b4j60l3mtp2k3x41s0w0000gn/T/ipykernel_10423/1099663790.py:2: FutureWarning: The default argument for metric will be removed in future version of Annoy. Please pass metric='angular' explicitly.\n",
      "  t = AnnoyIndex(f)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = 512 # Number of Dimensions\n",
    "t = AnnoyIndex(f)\n",
    "for i, j in combined_dict.items():\n",
    "    t.add_item(i, j) # Adding the key-value pair of the AnnoyIndex\n",
    "\n",
    "t.build(f) # Building 512 trees for AnnoyIndex, more the number of trees, more the memory consumed, better are the results of ANN algorithm\n",
    "t.save('image-search-tree.ann') # Saving the AnnoyIndex for faster reading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Search Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/xx/cq2k_b4j60l3mtp2k3x41s0w0000gn/T/ipykernel_12882/1782588339.py:1: FutureWarning: The default argument for metric will be removed in future version of Annoy. Please pass metric='angular' explicitly.\n",
      "  search_space = AnnoyIndex(512)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_space = AnnoyIndex(512)\n",
    "search_space.load('./image-search-tree.ann')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_image_search(query: str, num : int = 5):\n",
    "    query_vector = get_text_embedding(query) \n",
    "    ans = search_space.get_nns_by_vector(query_vector, num)\n",
    "    for i in ans:\n",
    "        image_path = all_combined_dict[i][0]\n",
    "        caption = all_combined_dict[i][1]\n",
    "        im = Image.open(image_path)\n",
    "        im.show()\n",
    "        print(caption, cosine_similarity(query_vector, combined_dict[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>                                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1 text_image_search(<span style=\"color: #808000; text-decoration-color: #808000\">'donkey hearing some secrets'</span>)                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2 </span>                                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">text_image_search</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">2</span>                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 1 </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">text_image_search</span>(query: <span style=\"color: #00ffff; text-decoration-color: #00ffff\">str</span>, num : <span style=\"color: #00ffff; text-decoration-color: #00ffff\">int</span> = <span style=\"color: #0000ff; text-decoration-color: #0000ff\">5</span>):                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 2 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>query_vector = get_text_embedding(query)                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 3 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>ans = search_space.get_nns_by_vector(query_vector, num)                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 4 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">for</span> i <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> ans:                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 5 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>image_path = all_combined_dict[i][<span style=\"color: #0000ff; text-decoration-color: #0000ff\">0</span>]                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">NameError: </span>name <span style=\"color: #008000; text-decoration-color: #008000\">'get_text_embedding'</span> is not defined\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92m<module>\u001b[0m:\u001b[94m1\u001b[0m                                                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1 text_image_search(\u001b[33m'\u001b[0m\u001b[33mdonkey hearing some secrets\u001b[0m\u001b[33m'\u001b[0m)                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2 \u001b[0m                                                                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92mtext_image_search\u001b[0m:\u001b[94m2\u001b[0m                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 1 \u001b[0m\u001b[94mdef\u001b[0m \u001b[92mtext_image_search\u001b[0m(query: \u001b[96mstr\u001b[0m, num : \u001b[96mint\u001b[0m = \u001b[94m5\u001b[0m):                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 2 \u001b[2m│   \u001b[0mquery_vector = get_text_embedding(query)                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 3 \u001b[0m\u001b[2m│   \u001b[0mans = search_space.get_nns_by_vector(query_vector, num)                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 4 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mfor\u001b[0m i \u001b[95min\u001b[0m ans:                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 5 \u001b[0m\u001b[2m│   │   \u001b[0mimage_path = all_combined_dict[i][\u001b[94m0\u001b[0m]                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mNameError: \u001b[0mname \u001b[32m'get_text_embedding'\u001b[0m is not defined\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "text_image_search('donkey hearing some secrets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
